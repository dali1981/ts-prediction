{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "def2842a",
      "metadata": {},
      "source": [
        "# Transformer Workflow Walkthrough\n",
        "\n",
        "This notebook demonstrates how to scaffold data ingestion, tokenization, and experiment configuration using the `transformers` toolkit. It builds a synthetic dataset, registers it in a catalog, and prepares both PatchTST and fusion model configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "466e4235",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(PosixPath('notebooks/_tmp/synthetic.csv'),\n",
              " PosixPath('notebooks/_tmp/brooks_vocab.json'))"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "from trading_transformers.data import DataCatalog, DataSource\n",
        "from trading_transformers.features import ContinuousFeatureBuilder, BrooksTokenizer\n",
        "from trading_transformers.tokenizers import BrooksTokenVocabulary\n",
        "\n",
        "TMP_ROOT = Path('notebooks/_tmp')\n",
        "TMP_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "CSV_PATH = TMP_ROOT / 'synthetic.csv'\n",
        "VOCAB_PATH = TMP_ROOT / 'brooks_vocab.json'\n",
        "\n",
        "# Synthetic OHLCV data\n",
        "synthetic = pd.DataFrame({\n",
        "    'timestamp': pd.date_range('2024-01-01', periods=400, freq='D'),\n",
        "    'open': 100 + pd.Series(range(400)).mul(0.05),\n",
        "    'high': 101 + pd.Series(range(400)).mul(0.05),\n",
        "    'low': 99 + pd.Series(range(400)).mul(0.05),\n",
        "    'close': 100 + pd.Series(range(400)).mul(0.05) + 0.3,\n",
        "    'volume': 1_000_000,\n",
        "})\n",
        "\n",
        "# Brooks-style tokens + vocabulary\n",
        "brooks = BrooksTokenizer()\n",
        "synthetic['brooks_token'] = brooks.transform(synthetic)\n",
        "vocab = BrooksTokenVocabulary.from_sequences(synthetic['brooks_token'])\n",
        "vocab.to_json(VOCAB_PATH)\n",
        "\n",
        "synthetic.to_csv(CSV_PATH, index=False)\n",
        "\n",
        "catalog = DataCatalog(root=TMP_ROOT)\n",
        "catalog.register_source(DataSource(name='synthetic', path=CSV_PATH, fmt='csv'))\n",
        "\n",
        "CSV_PATH, VOCAB_PATH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e02e2d3c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>close</th>\n",
              "      <th>log_return</th>\n",
              "      <th>hl_range</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100.30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100.35</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.01994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100.40</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.01993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100.45</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.01992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100.50</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.01991</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    close  log_return  hl_range\n",
              "0  100.30         NaN       NaN\n",
              "1  100.35    0.000498   0.01994\n",
              "2  100.40    0.000498   0.01993\n",
              "3  100.45    0.000498   0.01992\n",
              "4  100.50    0.000498   0.01991"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Continuous feature inspection (optional)\n",
        "builder = ContinuousFeatureBuilder()\n",
        "features = builder.transform(synthetic)\n",
        "features[['close', 'log_return', 'hl_range']].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "00023ac0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ExperimentConfig(name='notebook_patchtst', data=DataConfig(source='synthetic', features=['open', 'high', 'low', 'close', 'volume'], target='close', lookback=64, horizon=8, batch_size=64, val_fraction=0.1, test_fraction=0.1, token_column=None, vocab_path=None), model={'type': 'patchtst', 'input_dim': 5}, optimizer=OptimizerConfig(lr=0.001, weight_decay=0.0001), trainer=TrainerConfig(max_epochs=5, accelerator='cpu', precision='32', gradient_clip_val=1.0, devices=None), output_dir=PosixPath('artifacts'))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from trading_transformers.training import DataConfig, ExperimentConfig, OptimizerConfig, TrainerConfig\n",
        "\n",
        "# PatchTST baseline configuration\n",
        "patch_data_cfg = DataConfig(\n",
        "    source='synthetic',\n",
        "    features=['open', 'high', 'low', 'close', 'volume'],\n",
        "    target='close',\n",
        "    lookback=64,\n",
        "    horizon=8,\n",
        "    batch_size=64,\n",
        ")\n",
        "patch_experiment = ExperimentConfig(\n",
        "    name='notebook_patchtst',\n",
        "    data=patch_data_cfg,\n",
        "    model={'type': 'patchtst', 'input_dim': 5},\n",
        "    optimizer=OptimizerConfig(lr=1e-3),\n",
        "    trainer=TrainerConfig(max_epochs=5, accelerator='cpu', precision='32'),\n",
        ")\n",
        "patch_experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "93a15194",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ExperimentConfig(name='notebook_fusion', data=DataConfig(source='synthetic', features=['open', 'high', 'low', 'close', 'volume'], target='close', lookback=64, horizon=8, batch_size=64, val_fraction=0.1, test_fraction=0.1, token_column='brooks_token', vocab_path='notebooks/_tmp/brooks_vocab.json'), model={'type': 'fusion', 'd_model': 128, 'nheads': 4, 'depth': 2}, optimizer=OptimizerConfig(lr=0.001, weight_decay=0.0001), trainer=TrainerConfig(max_epochs=5, accelerator='cpu', precision='32', gradient_clip_val=1.0, devices=None), output_dir=PosixPath('artifacts'))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fusion configuration leveraging Brooks tokens\n",
        "fusion_data_cfg = DataConfig(\n",
        "    source='synthetic',\n",
        "    features=['open', 'high', 'low', 'close', 'volume'],\n",
        "    target='close',\n",
        "    lookback=64,\n",
        "    horizon=8,\n",
        "    batch_size=64,\n",
        "    token_column='brooks_token',\n",
        "    vocab_path=str(VOCAB_PATH),\n",
        ")\n",
        "fusion_experiment = ExperimentConfig(\n",
        "    name='notebook_fusion',\n",
        "    data=fusion_data_cfg,\n",
        "    model={\n",
        "        'type': 'fusion',\n",
        "        'd_model': 128,\n",
        "        'nheads': 4,\n",
        "        'depth': 2,\n",
        "    },\n",
        "    optimizer=OptimizerConfig(lr=1e-3),\n",
        "    trainer=TrainerConfig(max_epochs=5, accelerator='cpu', precision='32'),\n",
        ")\n",
        "fusion_experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "dc7d10c7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Catalog saved to notebooks/_tmp/catalog.json\n",
            "Available archives (first five): ['SHARADAR_DAILY_3_1c00e922d0fc2ccdfae0e4c5271349a4', 'SHARADAR_SEP_2_0afbc06bfa7d2d5ebd28c43e0940ec30', 'SHARADAR_SF1_017f04a0d2ef7cc409f920be72167ada', 'SHARADAR_SF2_6ae86d850a382c2a8a24c5daa109c39b', 'SHARADAR_SF3_ce320d02f19d0b5d04c9557e0bc16680']\n"
          ]
        }
      ],
      "source": [
        "# Optionally auto-register real archives from ../data (SHARADAR bundles, etc.)\n",
        "from trading_transformers.data import auto_register_archives\n",
        "\n",
        "DATA_DIR = Path('../data')\n",
        "if DATA_DIR.exists():\n",
        "    auto_register_archives(catalog, DATA_DIR)\n",
        "    catalog_path = TMP_ROOT / 'catalog.json'\n",
        "    catalog.to_json(catalog_path)\n",
        "    print('Catalog saved to', catalog_path)\n",
        "    print('Available archives (first five):', catalog.list_archives()[:5])\n",
        "else:\n",
        "    print('No external data directory found; using synthetic catalog only.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "82576a00",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping extraction of SHARADAR_DAILY_3_1c00e922d0fc2ccdfae0e4c5271349a4 (607.1 MB)\n"
          ]
        }
      ],
      "source": [
        "if catalog.list_archives():\n",
        "    archive_name = catalog.list_archives()[0]\n",
        "    archive_path = Path(catalog.archives[archive_name].path)\n",
        "    size_mb = archive_path.stat().st_size / (1024 * 1024)\n",
        "    if size_mb < 250:\n",
        "        folder = catalog.extract_archive(archive_name)\n",
        "        print('Extracted to', folder)\n",
        "        sample_files = sorted(f.name for f in folder.glob('*'))[:5]\n",
        "        print('Sample files:', sample_files)\n",
        "    else:\n",
        "        print(f'Skipping extraction of {archive_name} ({size_mb:.1f} MB)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b9b899c4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'token_stats': {'total_tokens': 400,\n",
              "  'unique_tokens': 1,\n",
              "  'entropy': -0.0,\n",
              "  'top_tokens': [('bull|bodyNA|tailNA|trend_up', 400)]},\n",
              " 'sample_tokens': ['bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up',\n",
              "  'bull|bodyNA|tailNA|trend_up'],\n",
              " 'vocab_size': 3}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Diagnostics for fusion tokens\n",
        "from trading_transformers.evaluation.diagnostics import fusion_token_report\n",
        "fusion_report = fusion_token_report(synthetic, fusion_data_cfg)\n",
        "fusion_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d5313005",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\ud83d\udca1 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (mps), used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "/Users/mohamedali/trading_project/models/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "/Users/mohamedali/trading_project/models/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "\n",
            "  | Name     | Type           | Params | Mode \n",
            "----------------------------------------------------\n",
            "0 | backbone | FusionBackbone | 406 K  | train\n",
            "1 | head     | Linear         | 1.0 K  | train\n",
            "----------------------------------------------------\n",
            "407 K     Trainable params\n",
            "0         Non-trainable params\n",
            "407 K     Total params\n",
            "1.628     Total estimated model params size (MB)\n",
            "28        Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fc9a82c05014f67b8464e7ee8c2af42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |                                                                                | 0/? [00:00\u2026"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mohamedali/trading_project/models/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
            "/Users/mohamedali/trading_project/models/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
            "/Users/mohamedali/trading_project/models/.venv/lib/python3.13/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "398eab7e79804dff984063b956537b6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |                                                                                       | 0/? [00:00\u2026"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69959de682d949948969e43343dce1af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                     | 0/? [00:00\u2026"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f02353f842dc4ec6abda619fc0e9eda9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                     | 0/? [00:00\u2026"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f00f9007a8ce4d5085ca39ef250bb621",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                     | 0/? [00:00\u2026"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91579dadb3b6439b934c5e652317f6bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                     | 0/? [00:00\u2026"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "536948a309f74e4eb8541a6cc6855230",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                     | 0/? [00:00\u2026"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diagnostics from runner: {'token_stats': {'total_tokens': 400, 'unique_tokens': 1, 'entropy': -0.0, 'top_tokens': [('bull|bodyNA|tailNA|trend_up', 400)]}, 'sample_tokens': ['bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up', 'bull|bodyNA|tailNA|trend_up'], 'vocab_size': 3}\n"
          ]
        }
      ],
      "source": [
        "from trading_transformers.training import ExperimentRunner\n",
        "try:\n",
        "    import torch\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError:\n",
        "    print('Install torch and pytorch_lightning to run training demo.')\n",
        "else:\n",
        "    runner = ExperimentRunner(config=fusion_experiment, catalog=catalog)\n",
        "    trainer = runner.run()\n",
        "    if runner.report.get('token_stats', {}).get('total_tokens', 0) > 0:\n",
        "        print('Diagnostics from runner:', runner.report)\n",
        "    runner.test(trainer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "except ImportError:\n",
        "    plt = None\n",
        "\n",
        "metrics_path = None\n",
        "if 'runner' in locals():\n",
        "    candidate = runner.metrics_path()\n",
        "    if candidate and Path(candidate).exists():\n",
        "        metrics_path = Path(candidate)\n",
        "\n",
        "if metrics_path and metrics_path.exists():\n",
        "    history = pd.read_csv(metrics_path)\n",
        "    display(history.tail())\n",
        "    value_cols = [col for col in ['train_loss', 'val_loss'] if col in history.columns]\n",
        "    if value_cols and plt is not None:\n",
        "        fig, ax = plt.subplots(figsize=(6, 3))\n",
        "        history.plot(x='epoch', y=value_cols, marker='o', ax=ax)\n",
        "        ax.set_ylabel('Loss')\n",
        "        ax.set_title('Training vs Validation Loss')\n",
        "        plt.show()\n",
        "    elif plt is None:\n",
        "        print('matplotlib is not installed; install it to see training curves.')\n",
        "else:\n",
        "    print('Run the training cell above to generate metrics.csv (requires torch & PyTorch Lightning).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b01b105a",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "- Swap the synthetic dataset with your catalog source and rerun diagnostics (`fusion_token_report`).\n",
        "- Use the CLI: `python -m trading_trading_transformers.training --config transformers/configs/fusion.yaml --catalog notebooks/_tmp/catalog.json --diagnostics fusion_report.json`.\n",
        "- Feed model forecasts into `python -m trading_trading_transformers.backtest` for P&L assessment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edf87b50-aab6-4a26-8ead-fe8f8ad41155",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86bce855-6099-479e-8224-898d7034c5ce",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8cedd5-b530-4825-b239-c8e5fef54bc1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d38d84c9-83f8-45b0-bba4-1d22f1b8086e",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}