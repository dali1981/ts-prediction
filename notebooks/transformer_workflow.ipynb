{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Workflow Walkthrough\n",
        "\n",
        "This notebook demonstrates how to scaffold data ingestion, tokenization, and experiment configuration using the `transformers` toolkit. It builds a synthetic dataset, registers it in a catalog, and prepares both PatchTST and fusion model configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "from trading_transformers.data import DataCatalog, DataSource\n",
        "from trading_transformers.features import ContinuousFeatureBuilder, BrooksTokenizer\n",
        "from trading_transformers.tokenizers import BrooksTokenVocabulary\n",
        "\n",
        "TMP_ROOT = Path('notebooks/_tmp')\n",
        "TMP_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "CSV_PATH = TMP_ROOT / 'synthetic.csv'\n",
        "VOCAB_PATH = TMP_ROOT / 'brooks_vocab.json'\n",
        "\n",
        "# Synthetic OHLCV data\n",
        "synthetic = pd.DataFrame({\n",
        "    'timestamp': pd.date_range('2024-01-01', periods=400, freq='D'),\n",
        "    'open': 100 + pd.Series(range(400)).mul(0.05),\n",
        "    'high': 101 + pd.Series(range(400)).mul(0.05),\n",
        "    'low': 99 + pd.Series(range(400)).mul(0.05),\n",
        "    'close': 100 + pd.Series(range(400)).mul(0.05) + 0.3,\n",
        "    'volume': 1_000_000,\n",
        "})\n",
        "\n",
        "# Brooks-style tokens + vocabulary\n",
        "brooks = BrooksTokenizer()\n",
        "synthetic['brooks_token'] = brooks.transform(synthetic)\n",
        "vocab = BrooksTokenVocabulary.from_sequences(synthetic['brooks_token'])\n",
        "vocab.to_json(VOCAB_PATH)\n",
        "\n",
        "synthetic.to_csv(CSV_PATH, index=False)\n",
        "\n",
        "catalog = DataCatalog(root=TMP_ROOT)\n",
        "catalog.register_source(DataSource(name='synthetic', path=CSV_PATH, fmt='csv'))\n",
        "\n",
        "CSV_PATH, VOCAB_PATH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Continuous feature inspection (optional)\n",
        "builder = ContinuousFeatureBuilder()\n",
        "features = builder.transform(synthetic)\n",
        "features[['close', 'log_return', 'hl_range']].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trading_transformers.training import DataConfig, ExperimentConfig, OptimizerConfig, TrainerConfig\n",
        "\n",
        "# PatchTST baseline configuration\n",
        "patch_data_cfg = DataConfig(\n",
        "    source='synthetic',\n",
        "    features=['open', 'high', 'low', 'close', 'volume'],\n",
        "    target='close',\n",
        "    lookback=64,\n",
        "    horizon=8,\n",
        "    batch_size=64,\n",
        ")\n",
        "patch_experiment = ExperimentConfig(\n",
        "    name='notebook_patchtst',\n",
        "    data=patch_data_cfg,\n",
        "    model={'type': 'patchtst', 'input_dim': 5},\n",
        "    optimizer=OptimizerConfig(lr=1e-3),\n",
        "    trainer=TrainerConfig(max_epochs=5, accelerator='cpu', precision='32'),\n",
        ")\n",
        "patch_experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fusion configuration leveraging Brooks tokens\n",
        "fusion_data_cfg = DataConfig(\n",
        "    source='synthetic',\n",
        "    features=['open', 'high', 'low', 'close', 'volume'],\n",
        "    target='close',\n",
        "    lookback=64,\n",
        "    horizon=8,\n",
        "    batch_size=64,\n",
        "    token_column='brooks_token',\n",
        "    vocab_path=str(VOCAB_PATH),\n",
        ")\n",
        "fusion_experiment = ExperimentConfig(\n",
        "    name='notebook_fusion',\n",
        "    data=fusion_data_cfg,\n",
        "    model={\n",
        "        'type': 'fusion',\n",
        "        'd_model': 128,\n",
        "        'nheads': 4,\n",
        "        'depth': 2,\n",
        "    },\n",
        "    optimizer=OptimizerConfig(lr=1e-3),\n",
        "    trainer=TrainerConfig(max_epochs=5, accelerator='cpu', precision='32'),\n",
        ")\n",
        "fusion_experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optionally auto-register real archives from ../data (SHARADAR bundles, etc.)\n",
        "from trading_transformers.data import auto_register_archives\n",
        "\n",
        "DATA_DIR = Path('../data')\n",
        "if DATA_DIR.exists():\n",
        "    auto_register_archives(catalog, DATA_DIR)\n",
        "    catalog_path = TMP_ROOT / 'catalog.json'\n",
        "    catalog.to_json(catalog_path)\n",
        "    print('Catalog saved to', catalog_path)\n",
        "    print('Available archives (first five):', catalog.list_archives()[:5])\n",
        "else:\n",
        "    print('No external data directory found; using synthetic catalog only.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Peek into the first registered archive if present (non-destructive)\n",
        "if catalog.list_archives():\n",
        "    archive_name = catalog.list_archives()[0]\n",
        "    folder = catalog.extract_archive(archive_name)\n",
        "    print('Extracted to', folder)\n",
        "    sample_files = sorted(f.name for f in folder.glob('*'))[:5]\n",
        "    print('Sample files:', sample_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostics for fusion tokens\n",
        "from trading_transformers.evaluation.diagnostics import fusion_token_report\n",
        "fusion_report = fusion_token_report(synthetic, fusion_data_cfg)\n",
        "fusion_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training requires PyTorch Lightning and torch.\n",
        "# Uncomment once dependencies are installed.\n",
        "# from trading_transformers.training import ExperimentRunner\n",
        "# runner = ExperimentRunner(config=fusion_experiment, catalog=catalog)\n",
        "# trainer = runner.run()\n",
        "# trainer.test()\n",
        "# print('Diagnostics from runner:', runner.report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "- Swap the synthetic dataset with your catalog source and rerun diagnostics (`fusion_token_report`).\n",
        "- Use the CLI: `python -m trading_trading_transformers.training --config transformers/configs/fusion.yaml --catalog notebooks/_tmp/catalog.json --diagnostics fusion_report.json`.\n",
        "- Feed model forecasts into `python -m trading_trading_transformers.backtest` for P&L assessment.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}